---
layout: post
title: "10-min tutorial: Install a local LLM on Windows with Ollama"
image: "/content/images/ollama-tuto.jpg"
ref: localllmtuto
lang: en
date:   2025-07-11 00:30:00 -0400
categories:
  - Artificial Intelligence
  - Tutorials
tags:
  - Artificial Intelligence
  - AI
  - Large Language Model
  - LLM
  - Tutorial
  - Ollama
excerpt_separator: <!--more-->
---
![10-min tuto - Local LLM on Windows with Ollama](/content/images/ollama-tuto.jpg){: style="display: block; margin: 0 auto;"}

In the previous post, I suggested some approaches to help as many people as possible understand the latest innovations in AI. Now, it only makes sense to set a good example from a tech popularization perspective üôÇ

As the post title suggests, here‚Äôs a quick, simple, and effective tutorial to install your own large language model instance, or LLM, locally on a Windows PC using [Ollama](https://ollama.com/).
<!--more-->

---
***Foreword***: *This article, enhanced with improvements generated by AI ([Napkin](https://www.napkin.ia) , [Google AI](https://ai.google/), [Perplexity](https://www.perplexity.ai/)), is made available under the* <span xmlns:cc="http://creativecommons.org/ns#" ><i><a href="https://creativecommons.org/licenses/by-nd/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-ND 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nd.svg?ref=chooser-v1" alt=""></a></i></span>

---
# What is an LLM?
Great question, thanks for asking üòÄ
An LLM (Large Language Model) is an artificial intelligence model trained on massive amounts of text data to understand and generate human language, learning to predict the next words in a sequence.

Among the best-known models for the general public:

* Available through [ChatGPT](https://chatgpt.com/), OpenAI‚Äôs GPT models ([GPT-4o](https://openai.com/fr-FR/index/gpt-4o-system-card/), [o3 and o4-mini](https://openai.com/index/introducing-o3-and-o4-mini/)).
* [Grok](https://x.ai/grok), by xAI, fully integrated into X.
* [Google Gemini](https://deepmind.google/models/gemini/), featured in Google tools like Gmail, Google Drive, Google Docs, etc.
* [LLama](https://www.llama.com/) models from Meta, fully integrated into Facebook, Instagram, and WhatsApp, and open-sourced since LLama 2 in July 2023 ([GitHub repo](https://github.com/meta-llama/llama)).

From this core capability, the applications of these AI models are numerous:
<picture>
  <source srcset="/content/images/llm_apps_d_en.svg" media="(prefers-color-scheme: dark)">
  <source srcset="/content/images/llm_apps_en.svg" media="(prefers-color-scheme: light)">
  <img src="/content/images/llm_apps_en.svg" alt="Versatile applications of LLM" style="display: block; margin: 0 auto;">
</picture>

* **Advanced chatbots**
  * Automated customer support,
  * Interactive FAQ.
* **Large-scale data processing**
  * Summarization,
  * Extraction, refinement, and classification.
* **Automatic translation**
  * For my previous article, I was glad to have access to LLMs to translate Chinese academic papers!
* **Code generation**
  * Widely used for coding assistance, I use [GitHub Copilot](https://github.com/features/copilot) to harmonize [this blog‚Äôs Markdown code](https://github.com/RFC2784/rfc2784.github.io/blob/main/_posts/)!
* **Reasoning simulation**
  * The latest achievements in this area are really interesting. Besides the [DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1) model, I experiment a lot with [Gemini Pro 2.5‚Äôs Deep Search](https://support.google.com/gemini/answer/15719111?hl=fr&sjid=8249538743889943279-NA).

# 10 minutes flat to install an LLM on a Windows PC‚Äîis it possible?

## Really?

OK, you won‚Äôt be running inferences with hundreds of billions of parameters on your laptop, let alone commercial models, but you can easily run smaller, yet powerful enough LLMs for fun!

## What‚Äôs the point of a local LLM?

Personally, I see only benefits:
<picture>
  <source srcset="/content/images/llm_perks_d_en.svg" media="(prefers-color-scheme: dark)">
  <source srcset="/content/images/llm_perks_en.svg" media="(prefers-color-scheme: light)">
  <img src="/content/images/llm_perks_en.svg" alt="Advantages of a local LLM" style="display: block; margin: 0 auto;">
</picture>

* Access to a **wide range of recent open-source LLMs**, including reasoning models like [DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1) or [Qwen 3](https://github.com/QwenLM/Qwen3).
* **Guaranteed privacy**: once the model is on your PC, no data leaves your computer, protecting all exchanges with the model (texts, images).
* **No subscription** for available features.
* **Give a second life to older PCs**. For example, I repurposed an old laptop with a Core i5 and 16 GB RAM as a personal server with Ubuntu, and I run a small model on it!

## What are the components?

### Ollama

Available on Linux, MacOS, Windows, and Docker, this open-source software platform allows you to manage and run language models locally.

* Website: [https://ollama.com](https://ollama.com)
* Discord: [https://discord.com/invite/ollama](https://discord.com/invite/ollama)
* List of available models: [https://ollama.com/library](https://ollama.com/library)
  * You can sign up [here](https://ollama.com/signup) to publish and share your own models.
* GitHub repo: [https://github.com/ollama/ollama](https://github.com/ollama/ollama)

### Hollama - Webchat Client for Ollama

Also available on Linux, MacOS, Windows, and Docker, this app is a webchat client compatible with Ollama and OpenAI.

* GitHub repo: [https://github.com/fmaclen/hollama](https://github.com/fmaclen/hollama)
* Live demo: [https://hollama.fernando.is/](https://hollama.fernando.is/)

I chose Hollama for its ease of use, especially for loading new models directly through the graphical interface.

### System Requirements

No need for a dedicated graphics card or a Copilot+ PC; the following config is enough to get started!

| Component | Minimum configuration |
| :---- | :---- |
| Operating System | Microsoft Windows 10 or Windows 11, Home or Pro edition, up to date with the latest patches |
| CPU | 4 cores, Intel/AMD, x86-64 |
| RAM | 8GB |
| Storage | 10GB available |
| Internet connectivity | Required for installing components and downloading language models. |

I validated this tutorial on virtual machines using this minimum configuration, so you can proceed with confidence üòâ

### Language model used in the tutorial - gemma3:1b-it-qat
**gemma3:1b-it-qat** refers to a lightweight and efficient version of Google‚Äôs open-source [Gemma 3](https://deepmind.google/models/gemma/gemma-3/) model:

* <u>1 billion parameters (‚Äú1b‚Äù)</u>.
* <u>Instruction-tuned ("it")</u>: Optimized to follow instructions and respond conversationally, **this model is ideal for text generation, summarization, and translation**.
* <u>Quantization-Aware Trained ("qat")</u>: Trained to support quantization (e.g., 4-bit), which **greatly reduces memory requirements while maintaining quality close to larger models**.

## How do you do it? (Step-by-step guide)
### Open PowerShell as administrator
   * Right-click the Start menu and select ‚ÄúTerminal (Admin)‚Äù or ‚ÄúWindows PowerShell (Admin)‚Äù.
<img src="/content/images/pshell_adm.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

   * Click \[Yes\] to accept privilege elevation.

<img src="/content/images/pshell_adm_uac_en.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

### Install a local Ollama instance, install the GUI, and fetch the gemma3:1b-it-qat model in a single command
   
* Run this command:
{% highlight powershell %}
winget install --id=Ollama.Ollama --accept-source-agreements -e; winget install --id=FernandoMaclen.Hollama -e; ollama pull gemma3:1b-it-qat; shutdown -r -t 0
{% endhighlight %}<br/>

   * Once the package is downloaded and verified, the Ollama instance installation process begins, and the Ollama installer GUI appears.

<img src="/content/images/ollama_installer_ui.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

  * During the final steps of this first installation process, the Microsoft C++ 2015/2022 Redistributable may be installed automatically if not already present.

<img src="/content/images/cpp_install_en.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

  * At the end of the Ollama instance installation, a welcome window appears. Click \[Finish\] to close it.

<img src="/content/images/ollama_welcome.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

* With the local Ollama instance installed, the Hollama GUI installation proceeds automatically.

<img src="/content/images/hollama_installer.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

* Once this installation process is complete, the **gemma3:1b-it-qat** model download to the local Ollama instance starts automatically.

<img src="/content/images/gemma3_dl.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

* Once the model is downloaded, the system will automatically restart. After rebooting, you‚Äôll see the Hollama app icon on the Desktop.

<img src="/content/images/hollama_icon.jpg" alt="" style="display: block; margin: 0 auto;">
### Finalize installation
* Double-click the Hollama icon on the desktop to launch the GUI. A Windows Firewall notification appears; click \[Cancel\] to block incoming connections.

<img src="/content/images/hollama_fw_en.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

* Once the app starts, it opens directly in the settings interface.

<img src="/content/images/hollama_ui_en.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

* In the Servers section, select ‚ÄúOllama‚Äù in the ‚ÄúConnection type‚Äù dropdown, then click \[Add connection\].

<img src="/content/images/ollama_server_en.gif" alt="" style="display: block; margin: 0 auto;"><br/>

* Click \[Verify\]. If all goes well, the message ‚ÄúConnection has been verified and is ready to use.‚Äù appears in a green box at the top, and the ‚ÄúUse models from this server‚Äù box is checked automatically.

<img src="/content/images/ollama_verified_en.gif" alt="" style="display: block; margin: 0 auto;"><br/>

### Test your first prompt
* In the left menu, click \[New session\].

<img src="/content/images/hollama_new_session_en.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

* Make sure the model is present in the dropdown, write your prompt, click \[Run\]. Wait a few seconds, and voil√†‚Äîmagic üòÄ

<img src="/content/images/hollama_chat_en.webp" alt="" style="display: block; margin: 0 auto;"><br/>

### How to download other models with the GUI?
* Go back to the Hollama app settings.
* Under the "Ollama" connection settings, click the "Ollama's Library" link. In the new window, browse the available model catalog, copy the identifier of the model you want to load (e.g., **qwen3:0.6b**), then close this window.

<img src="/content/images/ollama_library_en.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

* Back in the "Ollama" connection settings, paste the model identifier into the "Pull model" textbox, then click the red button to start loading.

<img src="/content/images/ollama_pull_model_en.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

* An info message appears at the top to show progress. The new model will be loaded and ready to use when the message turns green.

<img src="/content/images/ollama_model_ok_en.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

* You can now select and use this new model in conversation sessions.

<img src="/content/images/ollama_model_list_en.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

### Completely remove installed components
* Open PowerShell as administrator \([see first step](#open-powershell-as-administrator)\),
* Run the following command to remove all installed components (apps and models):
{% highlight powershell %}
winget uninstall --id=Ollama.Ollama ; winget uninstall --id=FernandoMaclen.Hollama; winget source reset --force; rm $env:USERPROFILE\*ollama* -R; rm $env:USERPROFILE\AppData\Local\*ollama* -R; rm $env:USERPROFILE\AppData\Local\Temp -R; rm $env:USERPROFILE\AppData\Roaming\*ollama* -R; shutdown -r -t 0
{% endhighlight %}
* Once the components are uninstalled, the system will automatically

---

## Conclusion

In just a few minutes, you can now experiment with the power of local LLMs on your Windows PC, without any subscription or cloud dependency. Whether for learning, coding assistance, or simply exploring the latest advances in artificial intelligence, Ollama and Hollama offer a simple and accessible solution.

Don‚Äôt hesitate to try out different models and share your feedback or discoveries!