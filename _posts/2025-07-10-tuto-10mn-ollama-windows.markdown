---
layout: post
title: "10-mn tuto : installer un LLM local sur Windows avec Ollama"
image: "/content/images/ollama-tuto.jpg"
ref: localllmtuto
lang: fr
date:   2025-07-11 00:30:00 -0400
categories:
  - Artificial Intelligence
  - Tutorials
tags:
  - Intelligence Artificielle
  - IA
  - Grand Mod√®le de Langage
  - LLM
  - Tutoriel
  - Ollama
excerpt_separator: <!--more-->
---
![10-mn tuto - LLM local sur Windows avec Ollama](/content/images/ollama-tuto.jpg){: style="display: block; margin: 0 auto;"}

Dans le post pr√©c√©dent, j‚Äôai propos√© des angles d‚Äôapproche pour permettre au plus grand nombre d‚Äôappr√©hender les derni√®res innovations en termes d‚ÄôIA. Maintenant, il est somme toute logique que je donne le bon exemple sous l‚Äôangle de la vulgarisation techno üôÇ

Comme le sugg√®re le titre du post, je vous propose un tutoriel rapide, simple et efficace pour installer votre propre instance de grand mod√®le de langage, ou LLM, localement sur un PC sous Windows en utilisant [Ollama](https://ollama.com/).
<!--more-->

---
***Avant-propos***: *Cet article, augment√© d'am√©liorations g√©n√©r√©es avec des IA ([Napkin](https://www.napkin.ia) , [Google AI](https://ai.google/), [Perplexity](https://www.perplexity.ai/)), est mis √† disposition au travers de la license* <span xmlns:cc="http://creativecommons.org/ns#" ><i><a href="https://creativecommons.org/licenses/by-nd/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-ND 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nd.svg?ref=chooser-v1" alt=""></a></i></span>

---
# C'est quoi un LLM ?
Tr√®s bonne question, merci de l'avoir pos√©e üòÄ
Un LLM (Large Language Model) est un mod√®le d'intelligence artificielle entra√Æn√© sur d'√©normes quantit√©s de donn√©es textuelles pour comprendre et g√©n√©rer du langage humain, apprenant ainsi √† pr√©dire les mots suivants dans une s√©quence.

Parmi les mod√®les les plus connus du grand public :

* Disponibles au travers de [ChatGPT](https://chatgpt.com/), les mod√®les GPT de OpenAI ([GPT-4o](https://openai.com/fr-FR/index/gpt-4o-system-card/), [o3 et o4-mini](https://openai.com/index/introducing-o3-and-o4-mini/)).
* [Grok](https://x.ai/grok), par xAI, qui est pleinement int√©gr√© √† X.
* [Google Gemini](https://deepmind.google/models/gemini/), mis en avant au travers des outils Google tels que Gmail,Google Drive, Google Docs, etc‚Ä¶
* Les mod√®les [LLama](https://www.llama.com/) de Meta, qui sont pleinement int√©gr√©s √† Facebook, Instagram et WhatsApp, sont rendus Open Source depuis la sortie de LLama 2 en juillet 2023 ([D√©p√¥t GitHub](https://github.com/meta-llama/llama)).

√Ä partir de cette capacit√© fondamentale, les applications de ces mod√®les IA sont nombreuses :
<picture>
  <source srcset="/content/images/llm_apps_d.svg" media="(prefers-color-scheme: dark)">
  <source srcset="/content/images/llm_apps.svg" media="(prefers-color-scheme: light)">
  <img src="/content/images/llm_apps.svg" alt="Applications polyvalentes des LLM" style="display: block; margin: 0 auto;">
</picture>

* **Chatbots avanc√©s**
  * support client automatis√©,
  * FAQ interactive.
* **Traitement de donn√©es √† grand volume**
  * R√©sum√©s,
  * Extraction, raffinage puis classification.
* **Traduction automatique**
  * Pour mon article pr√©c√©dent, j‚Äô√©tais bien content d‚Äôavoir acc√®s √† des LLM pour traduire des articles universitaires chinois \!
* **G√©n√©ration de code informatique**,
  * Beaucoup utilis√© pour l‚Äôaide au codage, j‚Äôutilise [GitHub Copilot](https://github.com/features/copilot) pour harmoniser [le code Markdown de ce blog](https://github.com/RFC2784/rfc2784.github.io/blob/main/_posts/) \!
* **Simulation de raisonnement**
  * Les derni√®res √©volutions dans ce domaine sont vraiment int√©ressantes. Outre le mod√®le [DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1), j‚Äôexp√©rimente beaucoup avec l‚Äôoption de [Recherche Profonde de Gemini Pro 2.5](https://support.google.com/gemini/answer/15719111?hl=fr&sjid=8249538743889943279-NA).

# 10 minutes chrono pour installer un LLM sur un PC Windows, c‚Äôest possible !

## Vraiment ?

OK, c‚Äôest s√ªr que vous n‚Äôallez pas faire tourner des inf√©rences avec des centaines de milliards de param√®tres sur votre laptop, et encore moins des mod√®les commerciaux, mais pas de souci pour faire tourner des LLM plus modestes mais suffisamment performants pour s'amuser \!

## C‚Äôest quoi l‚Äôint√©r√™t d‚Äôun LLM local ?

Personnellement, je n‚Äôy vois que des avantages
<picture>
  <source srcset="/content/images/llm_perks_d.svg" media="(prefers-color-scheme: dark)">
  <source srcset="/content/images/llm_perks.svg" media="(prefers-color-scheme: light)">
  <img src="/content/images/llm_perks.svg" alt="Les avantages d'un LLM local" style="display: block; margin: 0 auto;">
</picture>

* Acc√®s √† une **multitude de LLM Open Source r√©cents**, y compris des mod√®les de raisonnement comme [DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1) ou [Qwen 3](https://github.com/QwenLM/Qwen3).
* **Confidentialit√© garantie** : une fois le mod√®le sur le PC, aucune donn√©e ne quitte l‚Äôordinateur, prot√©geant ainsi les donn√©es √©chang√©es avec le mod√®le (textes, images).
* **Pas d'abonnement** pour les fonctionnalit√©s disponibles.
* **Possibilit√© de redonner un second souffle √† des PC un peu moins r√©cents**. Pour exemple, j‚Äôai reconditionn√© un vieux laptop avec un Core-I5 et 16 GB de RAM en serveur perso avec une distri Ubuntu, et je fais tourner un petit mod√®le dessus \!

## C‚Äôest quoi les composants ?

### Ollama

Disponible sur Linux, MacOs, WIndows et Docker, cette plateforme logicielle Open Source permet la gestion et l‚Äôex√©cution locale de mod√®les de langage.

* Site web : [https://ollama.com](https://ollama.com)
* Discord : [https://discord.com/invite/ollama](https://discord.com/invite/ollama)
* Liste des mod√®les disponibles : [https://ollama.com/library](https://ollama.com/library)
  * Il est possible de s'inscrire [ici](https://ollama.com/signup) pour publier et partager ses propres mod√®les.
* D√©pot GitHub : [https://github.com/ollama/ollama](https://github.com/ollama/ollama)

### Hollama - Client Webchat pour Ollama

Disponible √©galement sur Linux, MacOs, WIndows et Docker, cette application est un client Webchat compatible Ollama et OpenAI.

* D√©pot GitHub : [https://github.com/fmaclen/hollama](https://github.com/fmaclen/hollama)
* Live demo : [https://hollama.fernando.is/](https://hollama.fernando.is/)

J‚Äôai choisi Hollama pour sa simplicit√© d‚Äôutilisation, notamment pour charger de nouveaux mod√®les directement au travers de l‚Äôinterface graphique.

### Pr√©requis syst√®me

Pas besoin d‚Äôune carte graphique d√©di√©e ou d‚Äôun PC Copilot \+, la config suivante vous permet de faire vos premi√®res armes \!

| Composant | Configuration minimale |
| :---- | :---- |
| Syst√®me d‚Äôexploitation | Microsoft Windows 10 ou Windows 11, √©dition Famille ou Professionnel, maintenu √† jour avec les derniers correctifs |
| CPU | 4 coeurs, Intel/AMD, x86-64 |
| RAM | 8GB |
| Stockage | 10GB disponibles |
| Connectivit√© Internet | Requise pour l‚Äôinstallation des composants et le t√©l√©chargement des mod√®les de langage. |

J‚Äôai valid√© ce tutoriel sur des machines virtuelles qui utilisent cette configuration minimale, vous pouvez y aller les yeux ferm√©s üòâ

### Mod√®le de langage utilis√© dans le tutoriel - gemma3:1b-it-qat
**gemma3:1b-it-qat** d√©signe un version l√©g√®re et efficace du mod√®le Open Source [Gemma 3](https://deepmind.google/models/gemma/gemma-3/) de Google:

* <u>1 milliard de param√®tres (‚Äú1b‚Äù)</u>.
* <u>Instruction-tuned ("it")</u> : Optimis√©e pour suivre des instructions et r√©pondre de mani√®re conversationnelle, **ce mod√®le est id√©al pour la g√©n√©ration de texte, le r√©sum√© et la traduction**.
* <u>Quantization-Aware Trained ("qat")</u> : Entra√Æn√© pour supporter la quantification (par exemple en 4 bits), ce qui **r√©duit consid√©rablement la m√©moire n√©cessaire tout en maintenant une qualit√© proche des mod√®les plus lourds**.

## C‚Äôest comment qu‚Äôon fait ? (Guide √©tape par √©tape)
### Ouvrez PowerShell en mode administrateur
   * Clic-droit sur le menu D√©marrer et s√©lectionnez ¬´ Terminal (Administrateur)‚ÄØ¬ª ou ¬´Windows ‚ÄØPowerShell (Admin)‚ÄØ¬ª.
<img src="/content/images/pshell_adm.jpg" alt="" style="display: block; margin: 0 auto;">

   * Cliquez sur \[Oui\] pour accepter l‚Äô√©l√©vation de privil√®ges.
   
<img src="/content/images/pshell_adm_uac.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

### Installez un instance locale Ollama, installez l‚Äôinterface graphique et r√©cup√©rez le mod√®le gemma3:1b-it-qat en une seule ligne de commande
   
* Ex√©cutez cette commande :
{% highlight powershell %}
winget install --id=Ollama.Ollama --accept-source-agreements -e; winget install --id=FernandoMaclen.Hollama -e; ollama pull gemma3:1b-it-qat; shutdown -r -t 0
{% endhighlight %}<br/>

   * Une fois le paquet t√©l√©charg√© et v√©rifi√©, le processus d‚Äôinstallation de l‚Äôinstance Ollama commence, et l‚Äôinterface graphique d‚Äôinstallation Ollama s‚Äôaffiche.

<img src="/content/images/ollama_installer_ui.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

  * Durant l‚Äôune derni√®res √©tapes de ce premier processus d‚Äôinstallation, l‚Äôinstallation du Redistribuable Microsoft C++ 2015/2022 peut s‚Äôeffectuer de mani√®re automatique s‚Äôil n‚Äôest pas pr√©sent sur le syst√®me.

<img src="/content/images/cpp_install.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

  * √Ä la fin du processus d‚Äôinstallation de l‚Äôinstance Ollama, une fen√™tre de bienvenue s'affiche. Vous pouvez cliquer sur \[Finish\] pour la fermer.

<img src="/content/images/ollama_welcome.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

* L‚Äôinstance locale Ollama √©tant install√©e, l‚Äôinstallation de l‚Äôinterface graphique Hollama s‚Äôeffectue de mani√®re automatique.

<img src="/content/images/hollama_installer.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

* Une fois ce processus d‚Äôinstallation termin√©, le t√©l√©chargement du mod√®le **gemma3:1b-it-qat** sur l‚Äôinstance locale Ollama se lance de mani√®re automatique.

<img src="/content/images/gemma3_dl.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

* Une fois le mod√®le t√©l√©charg√©, le syst√®me red√©marre automatiquement. Une fois red√©marr√©, vous pouvez alors constater que l‚Äôic√¥ne de l‚Äôapplication Hollama est disponible sur le Bureau.

<img src="/content/images/hollama_icon.jpg" alt="" style="display: block; margin: 0 auto;">
### Finalisez l‚Äôinstallation
* Double-cliquez sur l‚Äôic√¥ne Hollama pr√©sent sur le bureau pour lancer l‚Äôinterface graphique. Une notification du pare-feu Windows s‚Äôaffiche, cliquez sur \[Annuler\] pour bloquer les flux entrants.

<img src="/content/images/hollama_fw.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

* Une fois l‚Äôapplication d√©marr√©e, celle-ci s'ouvre directement dans les param√®tres de l‚Äôinterface.

<img src="/content/images/hollama_ui.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

* Dans la section Serveurs, s√©lectionnez ‚ÄúOllama‚Äù dans la liste d√©roulante ‚ÄúType de connexion‚Äù puis cliquez sur \[Ajouter une connexion\].

<img src="/content/images/ollama_server.gif" alt="" style="display: block; margin: 0 auto;"><br/>

* Cliquez sur \[V√©rifier\]. Si tout va bien le message ‚ÄúLa connexion a √©t√© v√©rifi√©e et est pr√™te √† √™tre utilis√©e.‚Äù s‚Äôaffiche dans un encart vert en haut de la fen√™tre, et la case ‚ÄúUtiiser les mod√®les de ce serveur‚Äù se coche automatiquement.

<img src="/content/images/ollama_verified.gif" alt="" style="display: block; margin: 0 auto;"><br/>

### Testez un premier prompt
* Dans le menu de gauche, cliquez sur \[Nouvelle session\].

<img src="/content/images/hollama_new_session.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

* Validez que le mod√®le est pr√©sent dans la liste d√©roulante, √©crivez votre prompt, cliquez sur \[Ex√©cuter\]. Patientez quelques secondes, et voil√†, magie üòÄ

<img src="/content/images/hollama_chat.gif" alt="" style="display: block; margin: 0 auto;"><br/>

### Comment t√©l√©charger d'autres mod√®les avec l'interface graphique ?
* Retournez dans les param√®tres de l'application Hollama.
* Sous les param√®tres de la connexion "Ollama", cliquez sur le lien "Biblioth√®que d'Ollama". Dans la nouvelle fen√™tre qui s'affiche, parcourez le catalogue de mod√®les disponible, copiez l'identifiant du mod√®le que vous souhaitez t√©l√©charger (exemple : **qwen3:0.6b**), puis fermez cette fen√™tre.

<img src="/content/images/ollama_library.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

* De retour dans les param√®tres de la connexion "Ollama", collez l'identifiant du mod√®le dans la boite de texte "Tirer le mod√®le", puis cliquez sur le bouton rouge pour lancer le t√©l√©chargement.

<img src="/content/images/ollama_pull_model.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

* Un message d'information s'affiche en haut de la fen√®tre pour montrer la progression. Le nouveau mod√®le est t√©l√©charg√© et pr√™t √† √™tre utilis√© quand le message passera au vert.

<img src="/content/images/ollama_model_ok.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

* Vous pouvez d√©sormais choisir et utiliser ce nouveau mod√®le dans les sessions de conversation.

<img src="/content/images/ollama_model_list.jpg" alt="" style="display: block; margin: 0 auto;"><br/>

### Supprimer enti√®rement les composants install√©s
* Ouvrez PowerShell en mode administrateur \([cf. 1√®re √©tape](#ouvrez-powershell-en-mode-administrateur)\),
* Ex√©cutez la commande suivante pour supprimer tous les composants install√©s (applications et mod√®les) :
{% highlight powershell %}
winget uninstall --id=Ollama.Ollama ; winget uninstall --id=FernandoMaclen.Hollama; winget source reset --force; rm $env:USERPROFILE\*ollama* -R; rm $env:USERPROFILE\AppData\Local\*ollama* -R; rm $env:USERPROFILE\AppData\Local\Temp -R; rm $env:USERPROFILE\AppData\Roaming\*ollama* -R; shutdown -r -t 0
{% endhighlight %}
* Une fois les composants d√©sinstall√©s, le poste va red√©marrer automatiquement.

---

# Conclusion

En quelques minutes, vous pouvez d√©sormais exp√©rimenter la puissance des LLM locaux sur votre PC Windows, sans abonnement ni d√©pendance au cloud. Que ce soit pour apprendre, obtenir de l‚Äôaide √† la r√©daction ou au codage, ou simplement explorer les derni√®res avanc√©es de l‚Äôintelligence artificielle, Ollama et Hollama offrent une solution simple et accessible.

N‚Äôh√©sitez pas √† tester diff√©rents mod√®les, √† partager vos retours ou vos d√©couvertes, et √† contribuer √† la communaut√© open source !